{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Intrusion Detection using Two-stage Ensemble Learning\n",
    "\n",
    "Welcome to this homework - This homework implements the two-stage ensemble learning algorithms for intrusion detection that describes in the paper \"Dolus: Cyber Defense using Pretense against DDoS Attacks in Cloud Platforms\".\n",
    "\n",
    "Intrusion detection is a software application or a system to detect networking malicious activity or policy violations. In this task, you will work on the open dataset \"KDD Cup 1999 Data\" to build a network intrusion detector, a predictive model capable of distinguishing between \"bad\" connections, called intrusions, attacks, or malicious, and \"good\" normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment. \n",
    "\n",
    "**In this homework, you will learn how to:**\n",
    "- Learn the basic concept of ensemble learning and ensemble learning implementation using XGBoost\n",
    "- Learn the idea of two-stage ensemble learning for intrusion detection\n",
    "- Implement the two-stage ensemble learning using XGBoost\n",
    "\n",
    "**References**\n",
    "\n",
    "[ICDCN18] Neupane, Roshan Lal, et al. \"Dolus: cyber defense using pretense against DDoS attacks in cloud platforms.\" Proceedings of the 19th International Conference on Distributed Computing and Networking. 2018.(http://faculty.missouri.edu/calyamp/publications/dolus-cyber-defense-icdcn18.pdf)\n",
    "\n",
    "[KDD2016] Chen, Tianqi, and Carlos Guestrin. \"Xgboost: A scalable tree boosting system.\" Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "let's first import all the packages that you will need during this homework.\n",
    "- [python(3.8)](https://www.python.org/) \n",
    "- [numpy(1.19.1)](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib(3.3.0)](http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- [scikit-learn(0.23.2)](https://scikit-learn.org/) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "- [XGBoost(1.1.1)](https://xgboost.readthedocs.io/en/latest/) is an open-source software library which provides a gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala.\n",
    "- [pandas(1.1.0)](https://pandas.pydata.org/) is an open-source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- [seaborn(0.10.1)](https://seaborn.pydata.org/) is an open-source, statistical data visualization library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd \n",
    "import platform\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer, LabelBinarizer, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "warnings.filterwarnings('ignore')\n",
    "# if platform.system() == \"Darwin\":\n",
    "#     matplotlib.use('TkAgg')\n",
    "# else:\n",
    "#     matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Explore the dataset ##\n",
    "\n",
    "We can download the dataset from the official [websites](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html). In this lab, we have download all the datasets and put them into kddcup_dataset/ folder. The dataset provides a full version and a $10\\%$ version. But, we found the In this lab $10\\%$ version is not balanced well. We provide another small version with better balanced labels to demonstrate the basic idea effectively. You can visit the KDD cup 1999 original [task descrption](http://kdd.ics.uci.edu/databases/kddcup99/task.html).\n",
    "\n",
    "In short, the data used in this task is the networking packets collected from seven weeks of network traffic. Our task is to identify which packets are \"normal\" packet? and which packets are \"attack\" packet. If it's attack, we need to classify them into these four categories,\n",
    "* DOS: denial-of-service, e.g. syn flood;\n",
    "* R2L: unauthorized access from a remote machine, e.g. guessing password;\n",
    "* U2R: unauthorized access to local superuser (root) privileges, e.g., various, \"buffer overflow\" attacks;\n",
    "* probing: surveillance and other probing, e.g., port scanning.\n",
    "\n",
    "For any machine learning question, we need to understand its features and labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Features\n",
    "\n",
    "First, we can load the dataset into pandas dataframe use pandas.read_csv() interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('kddcup_dataset/kddcup.data.small', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, each packet has 41 features and one label (\"label2\") that indicates the class/type of packet. We can add the features name to the dataframe. Each feature is defined in file kddcup_dataset/kddcup.names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\n",
    "    'duration',      # continuous, length (number of seconds) of the connection \n",
    "    'protocol_type', # discrete, type of the protocol, e.g. tcp, udp, etc. \n",
    "    'service',       # discrete, network service on the destination, e.g., http, telnet, etc. \n",
    "    'flag',          # discrete, normal or error status of the connection \n",
    "    'src_bytes',     # continuous, number of data bytes from source to destination \n",
    "    'dst_bytes',     # continuous, number of data bytes from destination to source \n",
    "    'land',          # discrete, 1 if connection is from/to the same host/port; 0 otherwise \n",
    "    'wrong_fragment',# continuous, number of ``wrong'' fragments \n",
    "    'urgent',        # continuous, number of urgent packets \n",
    "    'hot',           # continuous, number of ``hot'' indicators\n",
    "    'num_failed_logins',   # continuous, number of failed login attempts \n",
    "    'logged_in',           # discrete, 1 if successfully logged in; 0 otherwise \n",
    "    'num_compromised',     # continuous, number of ``compromised'' conditions \n",
    "    'root_shell',          # discrete, 1 if root shell is obtained; 0 otherwise\n",
    "    'su_attempted',        # discrete, 1 if ``su root'' command attempted; 0 otherwise\n",
    "    'num_root',            # continuous, number of ``root'' accesses \n",
    "    'num_file_creations',  # continuous, number of file creation operations\n",
    "    'num_shells',          # continuous, number of shell prompts\n",
    "    'num_access_files',    # continuous, number of operations on access control files\n",
    "    'num_outbound_cmds',   # continuous, number of outbound commands in an ftp session \n",
    "    'is_host_login',       # discrete, 1 if the login belongs to the ``host'' list; 0 otherwise \n",
    "    'is_guest_login',      # discrete, 1 if the login is a ``guest''login; 0 otherwise \n",
    "    'count',               # continuous, number of connections to the same host as the current connection in the past two seconds \n",
    "    'srv_count',           # continuous, number of connections to the same service as the current connection in the past two seconds \n",
    "    'serror_rate',         # continuous, % of connections that have ``SYN'' errors \n",
    "    'srv_serror_rate',     # continuous, % of connections that have ``SYN'' error\n",
    "    'rerror_rate',         # continuous, % of connections that have ``REJ'' errors \n",
    "    'srv_rerror_rate',     # continuous, % of connections that have ``REJ'' errors\n",
    "    'same_srv_rate',       # continuous, % of connections to the same service \n",
    "    'diff_srv_rate',       # continuous, % of connections to different services\n",
    "    'srv_diff_host_rate',  # continuous, % of connections to different hosts\n",
    "    'dst_host_count',      # continuous, number of connections to the same host as the current connection in the past two seconds \n",
    "    'dst_host_srv_count',  # continuous, number of connections to the same service as the current connection in the past two seconds \n",
    "    'dst_host_same_srv_rate', # continuous % of connections to the same service \n",
    "    'dst_host_diff_srv_rate', # continuous, % of connections to different services \n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', \n",
    "    'dst_host_serror_rate', \n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', \n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'label2'                      # discrete, class of this packet\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data format after adding the feature name\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Labels\n",
    "\n",
    "The label type is defined in file \"kddcup_dataset/training_attack_types\", The orginal dataset has 22 attack types, and we need to map 22 types into 4 types (dos, u2r, r2l, probe). In our task, we just need to classify each attack into four categories. \n",
    "\n",
    "We create this mapping table. We also add the \"normal\" type. Then, there are five types. So, our final task is to classify each packet into these five categories. \n",
    "\n",
    "We can check it by,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet_class = {\n",
    "    'normal': 'normal',\n",
    "    'back': 'dos',\n",
    "    'buffer_overflow': 'u2r',\n",
    "    'ftp_write': 'r2l',\n",
    "    'guess_passwd': 'r2l',\n",
    "    'imap': 'r2l',\n",
    "    'ipsweep': 'probe',\n",
    "    'land': 'dos',\n",
    "    'loadmodule': 'u2r',\n",
    "    'multihop': 'r2l',\n",
    "    'neptune': 'dos',\n",
    "    'nmap': 'probe',\n",
    "    'perl': 'u2r',\n",
    "    'phf': 'r2l',\n",
    "    'pod': 'dos',\n",
    "    'portsweep': 'probe',\n",
    "    'rootkit': 'u2r',\n",
    "    'satan': 'probe',\n",
    "    'smurf': 'dos',\n",
    "    'spy': 'r2l',\n",
    "    'teardrop': 'dos',\n",
    "    'warezclient': 'r2l',\n",
    "    'warezmaster': 'r2l',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use map function to map label into these five categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label2'] = data.label2.apply(lambda t: packet_class[t[:-1]])\n",
    "data.label2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because in this homework, we will demonstrate the idea of two-stage ensemble learning. We add another label-\"label1\" to indicate whehter it's normal packet or attack packet, so it's binary class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet_class2 = {\n",
    "    'normal': 'normal',\n",
    "    'dos': 'attack',\n",
    "    'u2r': 'attack',\n",
    "    'r2l': 'attack',\n",
    "    'probe': 'attack',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label1'] = data.label2.apply(lambda t: packet_class2[t])\n",
    "data.label1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can observe that there are two labels - \"label1\" and \"label2\" in the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Explore features\n",
    "\n",
    "We can explore the features to understand their distributions and relevant patterns, which can help us for feature selections. Here, we demonstrate some commons methods to explore the features. You may also try other methods as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(data, features):\n",
    "    data[features].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Counts by protocol type\")\n",
    "plot_bar(data, 'protocol_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.title(\"Counts by service\")\n",
    "plot_bar(data, 'service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.title(\"Counts by flag\")\n",
    "plot_bar(data, 'flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore the correlation among different features,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = data.dropna('columns')\n",
    "df_corr = df_corr[[col for col in df_corr if df_corr[col].nunique() > 1]]\n",
    "corr = df_corr.corr()\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(corr)\n",
    "plt.title(\"Features correlations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the figure above, the lighter color indicates a higher correlation between two features, and redundant features can be eliminated by higher correlations. Although current advanced machine learning models like deep neural network (DNN) can learn useful features by themselves, it's also very useful to understand the basic theory of feature selections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Ensemble Learning ##\n",
    "\n",
    "In this section, we will briefly introduce the concept of ensemble learning. In short, Ensemble learning uses multiple learning models to imporve the predictive performance than a single learning model. Common ensemble learning methods are bagging, boosting, and stacking. \n",
    "\n",
    "* __Bagging__: Bootstrap Aggregation or Bagging has two distinct features which define its training and prediction. For training it leverages a Bootstrap procedure to separate the training data into different random subsamples, which different iterations of the model use to train on. For prediction, a bagging classifier will use the prediction with the most votes from each model to produce its output and a bagging regression will take an average of all models to produce an output. Bagging is typically applied to high variance models such as Decision Trees and the Random Forest algorithm is a very close variation on bagging.\n",
    "\n",
    "* __Boosting__: The core definition of boosting is a method that converts weak learners to strong learners, and is typically applied to trees. More explicitly, a boosting algorithm adds iterations of the model sequentially, adjusting the weights of the weak-learners along the way. This reduces bias from the model and typically improves accuracy. Popular boosting algorithms are AdaBoost, Gradient Tree Boosting, and XGBoost, which we’ll focus on here.\n",
    "\n",
    "* __Stacking__: A Stacking model is a “meta-model” which leverages the outputs from a collection of many, typically significantly different, models as input features. For instance, this allows you to train a K-NN, Linear Regression, and Decision Tree with all of your training data, then take those outputs and merge them with a Logistical Regression. The idea is that this can reduce overfitting and improve accuracy.\n",
    "\n",
    "In this lab, we foucus on the Boosting methods using XGBoost. XGBoost stands for “Extreme Gradient Boosting” that is based on gradient boosting algorithm. In this Lab, you will learn to how to use XGBoost for attack detection (i.e., classificaiton tasks). More information about XGBoost, please visit visited XGBoost paper [KDD2016](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf) and its [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/model.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Two-stage Ensemble Learning\n",
    "\n",
    "The two-stage ensemble learning method is described in the paper [ICDCN18](http://faculty.missouri.edu/calyamp/publications/dolus-cyber-defense-icdcn18.pdf). \n",
    "\n",
    "The two-stage ensemble learning has two stages: \n",
    "* __Detection Stage__: detect whether a packet is an attack packet or a normal packet. It's a binary classification task ('normal', 'attack')\n",
    "* __Classification Stage__: classify each attack into these four categories ('dos', 'u2r', 'r2l', 'probe')\n",
    "\n",
    "As we know, the first stage is a binary classification task and the second stage is multi-classes classifications task. Hence, the task in the first stage is easier than the task in the second stage. Theoretically, we can use fewer features and a simpler model in stage 1 than stage 2. \n",
    "\n",
    "####  Why do we use two-stage ensemble learning?\n",
    "\n",
    "In fact, we can just use one model to classify each packet into 5 classes ('normal', 'dos', 'u2r', 'r2l', 'probe') in one stage. The reason for proposing the two-stage ensemble learning is for computational efficiency. \n",
    "\n",
    "In the real-time intrusion detection system (IDS), the packets collectors are distributed around the different hosts, and the collectors need to send the collected packets to the central server to detect attacks. Because the ratio of attack packets is much smaller than the normal packets, most of the time of computation resources and networking transmission resources are wasted in processing \"normal packet\", which may also cause real attack packets cannot be processed in real-time. \n",
    "\n",
    "Our two-stage ensemble learning is to deal with these issues. Because we consider that detection task (binary classification) is simpler than the classification task (multi-classes) classification. For the easier task, we can use less information (or features) to train a model and detect using a fewer amount of dataset; and for the harder task, we can use more information to train and test. \n",
    "\n",
    "The whole two-stage ensemble learning can be divided into two phrases,\n",
    "\n",
    "\n",
    "#### Training phrase\n",
    "During the trainging phrase, we train two model separately: one is __detection model__ to detect attack or not; one is __classification model__ to classify attacks. In the first task, we need to use as little features and simpler as possible at situations of similar performance. \n",
    "\n",
    "#### Testing phrase\n",
    "During the testing phrase, we always use __detection model__  with less features datasets to detect attack in the beginning. When we detect there is an attack. The server will to request packet client to ask for more features datasets, then we will use __classification model__ to classify attacks. \n",
    "\n",
    "In our lab, we only foucs on the machine learning model. And, when to shifing from __deteciton model__ to __classification model__ and when to make a request for more features dataset belongs to system level, which is out of scope of this lab.\n",
    "\n",
    "Next section, we will implement the two-stage ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Two-stage ensemble learning implementaion\n",
    "\n",
    "In the section, we will implement the two-stage ensemble learning method\n",
    "\n",
    "### 5.1 - Detection Model\n",
    "\n",
    "For detection model, we use \"label1\" (normal, attack) for binary classification,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Prepare training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding string features into integer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if data[column].dtype == np.object and column != 'label1' and column != 'label2':\n",
    "        encoded = LabelEncoder()\n",
    "        \n",
    "        encoded.fit(data[column])\n",
    "        data[column] = encoded.transform(data[column])\n",
    "\n",
    "# Encoding label into integer \n",
    "label1_encoded = LabelEncoder()\n",
    "label1_encoded.fit(data['label1'])\n",
    "data['label1'] = label1_encoded.transform(data['label1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare inputs $X$ and labels $Y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our X, which drop label1 and label 2\n",
    "X = data.drop(['label1', 'label2'], axis=1)\n",
    "Y = data.label1\n",
    "\n",
    "# Split our train and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "        \n",
    "# We can also rescale some features\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X_train[['dst_bytes','src_bytes']] = scaler.fit_transform(X_train[['dst_bytes','src_bytes']])\n",
    "X_test[['dst_bytes','src_bytes']] = scaler.transform(X_test[['dst_bytes','src_bytes']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Select features\n",
    "\n",
    "At the situation of similar performance, select as fewer features as possible,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['protocol_type', 'src_bytes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model\n",
    "detector = XGBClassifier(max_depth=3, n_estimators=10, random_state=42, verbosity=1)\n",
    "\n",
    "# train the model\n",
    "detector.fit(X_train[selected_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Model testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "print(\"The trianing accuracy: \", detector.score(X_train[selected_features], y_train))\n",
    "print(\"The testing accuracy: \",detector.score(X_test[selected_features],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we can achieve $96\\%$ accuracy in test datasets with only 2 features that effectively saves computational and networking resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Classification Model\n",
    "\n",
    "For classification model, our task is to classify each attack into four categories ('dos', 'u2r', 'r2l', 'probe'). Hence, we just need to consider the attack dataset, Hence, we need to drop \"normal\" labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Prepare training and testing datasets\n",
    "\n",
    "We need to drop the rows with label \"normal\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls = data[data.label2 != 'normal']\n",
    "data_cls.label2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding label into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2_encoded = LabelEncoder()\n",
    "label2_encoded.fit(data_cls['label2'])\n",
    "data_cls['label2'] = label2_encoded.transform(data_cls['label2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare inputs $X$ and labels $Y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our X, which drop label1 and label 2\n",
    "X = data_cls.drop(['label1', 'label2'], axis=1)\n",
    "Y = data_cls.label2\n",
    "\n",
    "# Split our train and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "        \n",
    "# We can also rescale some features\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X_train[['dst_bytes','src_bytes']] = scaler.fit_transform(X_train[['dst_bytes','src_bytes']])\n",
    "X_test[['dst_bytes','src_bytes']] = scaler.transform(X_test[['dst_bytes','src_bytes']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Select features\n",
    "\n",
    "We can just use same features as detector model to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['protocol_type', 'src_bytes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model\n",
    "classifier = XGBClassifier(max_depth=3, n_estimators=10, random_state=42, verbosity=1)\n",
    "\n",
    "# train the model\n",
    "classifier.fit(X_train[selected_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "print(\"The training accuracy: \", classifier.score(X_train[selected_features], y_train))\n",
    "print(\"The testing accuracy: \",classifier.score(X_test[selected_features], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the confusion matrix to check performance for each class,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test[selected_features])\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Conclusion\n",
    "\n",
    "Hence, if we use same features in detector and classification model, the detector model (simple model) works better than classification model. If we want to imporve the performance of classification model, we need to add more features or increase the complexity of the model. You need to implement this in your homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Explain the main purposes of two-stages ensemble learning?\n",
    "2. As we mention in this lab, the original $10\\%$ dataset is not balanced well. Can you explain the effect of imbalanced datasets? and how to deal with imbalanced datasets?\n",
    "3. Take research about XGBoost, can you briefly introduce what is XGBoost and differences between XGBoost and GBDT?\n",
    "4. Using lab results in section 5 as our baseline, can you improve the testing accuracy performance of the classification model by adding more features or complexity of ensemble models or any other methods. But, the model should be restricted in XGBoost. Explain how you improve the performance (e.g., the features you used, parameters you adjusted).  Please compyu submit your executable jupyter notebook file (*.ipynb) with output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
