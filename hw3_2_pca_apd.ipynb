{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Network-Wide Anomaly Event Detection and Diagnosis (PCA-APD)\n",
    "\n",
    "Welcome to network-wide anomaly detection homework - This homework implements the algorithms described in the paper of \"Network-Wide Anomaly Event Detection and Diagnosis With perfSONAR\". In the homework 3.2, we will learn the second part of this paper, PCA-APD for network-wide performance detection to detect corrlected anomalies in the multiple measurement traces. \n",
    "\n",
    "**In this homework, you will learn:**\n",
    "- The basic theory and dimension reduction and how to use Principal component analysis (PCA) and Singular value decomposition (SVD)  algorithms for dimension reduction;\n",
    "- The idea of using dimension reduction to separate correlated anomalies and uncorrelated anomalies from multiple measurements traces.\n",
    "- How to combine SVD and our APD algorithm for correlated and uncorrelated anomalies detection.\n",
    "\n",
    "In homework 3.1, you have learned APD agorithm to detected anomalies in single trace. In this homework, you will learn how to SVD and APD algorithm to detect anomalies in multiple traces. More specificly, we need to detect correlated anomalies and correlated anomalies in multiple traces. Correlated anomalies are anomalies occurred at same time in different traces, which are more important than uncorrelated anomalies. \n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "[TNSM2016] Zhang, Yuanxun, Saptarshi Debroy, and Prasad Calyam. \"Network-wide anomaly event detection and diagnosis with perfsonar.\" IEEE Transactions on Network and Service Management 13.3 (2016): 666-680. (http://faculty.missouri.edu/calyamp/publications/networkwide-apd-tnsm16.pdf)\n",
    "\n",
    "[DRCN2015] Zhang, Yuanxun, Prasad Calyam, Saptarshi Debroy, and Mukundan Sridharan. \"PCA-based network-wide correlated anomaly event detection and diagnosis.\" In 2015 11th International Conference on the Design of Reliable Communication Networks (DRCN), pp. 149-156. IEEE, 2015.\n",
    "(http://faculty.missouri.edu/calyamp/publications/pca-apd-drcn15.pdf)\n",
    "\n",
    "[MASCOTS2010] Calyam, Prasad, et al. \"Ontimedetect: Dynamic network anomaly notification in perfsonar deployments.\" 2010 IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems. IEEE, 2010. (http://faculty.missouri.edu/calyamp/publications/ontimedetect_mascots10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 - Packages ##\n",
    "let's first import all the packages that you will need during this homework.\n",
    "- [numpy(1.19.1)](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib(3.3.0)](http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- [pandas(1.1.0)](https://pandas.pydata.org/) is an open-source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- [Scikit-learn(0.23.2)](https://scikit-learn.org/stable/) Scikit-learn is a free software machine learning library for the Python programming language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import matplotlib\n",
    "from sklearn.decomposition import PCA\n",
    "import platform\n",
    "import os\n",
    "from IPython.display import display\n",
    "# if platform.system() == \"Darwin\":\n",
    "#     matplotlib.use('TkAgg')\n",
    "# else:\n",
    "#     matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# set current path\n",
    "abspath = os.path.abspath(\"__file__\")\n",
    "dname = os.path.dirname(abspath)\n",
    "os.chdir(dname)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Principal component analysis (PCA) Tutorial##\n",
    "\n",
    "In this section, we will reivew the theory of PCA for dimension reduction, and relevant implementation. \n",
    "\n",
    "PCA is a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction. Here, we show a toy example to demonstrate how to use PCA in Scikit-learn for dimension reduction. Please, find more inforamtion about PCA through this link (https://scikit-learn.org/stable/modules/decomposition.html#pca and https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1 Generate random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 2 Use Scikit-Learn's PCA estimator to get principal components\n",
    "\n",
    "Here, we extract two principal components from orginal datasets. The first principal component captures the main variance, the second principal component captures the second variance, principal component is orthogonal to each other, and, the number of principal components should be smaller or equal to the dimension of orginal datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, We can print the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show its variance as follows, the first components capture the main variance, and the second components capture the second important variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Visualizing principal components\n",
    "To see what these numbers mean, we can visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_visualizing(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    components_visualizing(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the figure above, these vectors represent the principal components of the data, and the length of the components is an indication of how \"important\" that axis is in describing the distribution of the data. More precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the \"principal components\" of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. 4 PCA for dimensionality reduction\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "For example, we use our toy dataset that is two dimensional dataset. Then, we want to reduce it into one dimensional space that preserves the maximum data variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we just extract first component\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Conclusion\n",
    "Congrats on finishing the tutorial using PCA for dimension reduction. Next, you will learn how to apply the PCA to anomaly detection. And you also will learn using SVD to dimension reduction in anomaly detection, which performs similarly as PCA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Revisit our APD algorithm ##\n",
    "\n",
    "APD is adapative plateau detector to detect anomalies in a single measurement trace. Here, we revisit the key functions of the adaptive plateau detector in homework 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = os.getcwd() + '/traffic_dataset/time_series.csv'\n",
    "print(dataset_folder)\n",
    "df = pd.read_csv(dataset_folder, header=0, parse_dates=[0])\n",
    "# Change the index to timstampe\n",
    "df = df.set_index(['ts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Define our anomalies visulaization function that is same as homework 3.1, which is used to plot anomalies in a time-series chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_anomalies(anomalies, df, label='anomalies'):\n",
    "    # 1. Plot original time series plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Delay', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    ax.tick_params(axis='x', labelsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    plt.plot(df.index, df.delay, 'k', label=\"_nolegend_\")\n",
    "\n",
    "    # 2. Plot anomalies\n",
    "    if anomalies:\n",
    "        for i in anomalies:\n",
    "            plt.scatter(i, df.loc[i]['delay'], marker='x', s=150, c='r')\n",
    "\n",
    "        plt.scatter(i, df.loc[i]['delay'], marker='x', s=150, c='r', label=label)\n",
    "    plt.legend(loc='best', fontsize=12, edgecolor='black', fancybox=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Define our Adaptive Pleateau Detector (APD) algorithm that is same as homework 3.1, which is used to detect anoamlies dynamicaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "np.random.seed(0)\n",
    "\n",
    "class APD(object):\n",
    "    def __init__(self, state='NE', window_size=20, dataset=None, sensitivity=3, trigger_threshold=6):\n",
    "        # 'NE': no event; 'ED': event detected; 'EI': event impending\n",
    "        # The initial state is 'NE' (no event)\n",
    "        self.state = state\n",
    "        \n",
    "        # the counter defines the number of detected events\n",
    "        self.count = 0\n",
    "        \n",
    "        # define the window size\n",
    "        self.window_size = 20\n",
    "        \n",
    "        # the data in the window\n",
    "        self.sliding_window = deque()\n",
    "        \n",
    "        # keep the dataset (dataframe)\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        # define the sensitivity\n",
    "        self.sensitivity = 3\n",
    "        \n",
    "        # store the anomalies detected\n",
    "        self.anomalies = []\n",
    "        \n",
    "        # keep the count of trigger\n",
    "        self.trigger_count = 0\n",
    "        \n",
    "        # keep the trigger duration threshold\n",
    "        self.trigger_threshold = trigger_threshold\n",
    "        \n",
    "        # initialize windows\n",
    "        self.initialize_windows()\n",
    "    \n",
    "    def initialize_windows(self):\n",
    "        # similar as previous function\n",
    "        initial_values = []\n",
    "  \n",
    "        for i in range(self.window_size):\n",
    "            v = np.mean(np.random.choice(self.dataset['delay'].values, 100))\n",
    "            self.sliding_window.append(v)\n",
    "            \n",
    "   \n",
    "    def compute_mean_std(self, dataset):\n",
    "        mean = np.mean(dataset)\n",
    "        std = np.std(dataset)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def compute_threshold(self):\n",
    "        # compute threshold \n",
    "        mean, std = self.compute_mean_std(self.sliding_window)\n",
    "        up = mean + self.sensitivity * std\n",
    "        low = mean - self.sensitivity * std\n",
    "\n",
    "        threshold = {'up': up, 'low': low}\n",
    "        return threshold\n",
    "\n",
    "    def detect(self):\n",
    "        # iterate the whole dataset \n",
    "        for index, row in self.dataset.iterrows():\n",
    "            delay = row['delay']\n",
    "\n",
    "            # 1. use function append() and popleft to udpate sliding window\n",
    "            self.sliding_window.append(delay)\n",
    "            self.sliding_window.popleft()\n",
    "            \n",
    "            assert(len(self.sliding_window) == self.window_size)\n",
    "\n",
    "            # 2. compute threshold within the window, update the threshold when there is no trigger\n",
    "            if self.trigger_count == 0:\n",
    "                threshold = self.compute_threshold()\n",
    "            \n",
    "            # 3. use the logic to implement trigger duration scheme\n",
    "            if delay > threshold['up'] or delay < threshold['low']:\n",
    "                \n",
    "                self.trigger_count += 1\n",
    "                if self.state == 'NE' and self.trigger_count >= 0.75 * self.trigger_threshold:\n",
    "                    self.state = 'EI'\n",
    "                elif self.state == 'EI' and self.trigger_count == self.trigger_threshold:\n",
    "                    self.state = 'ED'\n",
    "                    self.anomalies.append(index)\n",
    "            else:\n",
    "                if self.trigger_count > 0:\n",
    "                    self.trigger_count -= 1\n",
    "                    \n",
    "                if self.state == 'ED' and self.trigger_count < self.trigger_threshold:\n",
    "                    self.state = 'EI'\n",
    "                elif self.state == 'EI' and self.trigger_count < int(0.75 * self.trigger_threshold):\n",
    "                    self.state = 'NE'\n",
    "            \n",
    "        return self.anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FUNCTION: Trigger duration scheme\n",
    "apd = APD(dataset=df)\n",
    "anomalies = apd.detect()\n",
    "\n",
    "# 2. visulize anomlies\n",
    "visualize_anomalies(anomalies, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In APD, we have learned how to detect anomalies in single trace as shown in figure above. But, it cannot tell it is a correlated or uncorrelated anomalies. However, in network-wide anomaly detection, the correlated anomalies are more important than uncorrelated anomalies. In next section, we will learn how to use PCA-APD to detect correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - PCA-APD  ##\n",
    "\n",
    "In this section, we will learn using PCA-APD to detect correlated anomalies and uncorrelated anomalies from multiple measurement traces. \n",
    "\n",
    "- Correlated anomalies are anomalies occurred at the same time in different traces;\n",
    "- Uncorrelated anomalies are anomalies that occurred at a certain time at a single trace. \n",
    "\n",
    "####  Why do we need to detect correlated anomalies?\n",
    "\n",
    "Correlated anomalies are usually more important than the uncorrelated anomaly. Because, uncorrelated anomalies maybe some random events that occurred at a single trace, which won't have an impact on the whole network. However, the correlated anomalies occur in multiple traces at the same time, which will cause some impacts on the whole network, and it is very helpful for root-cause analysis.\n",
    "\n",
    "The basic idea of PCA-APD algorithms is to use SVD to separate the correlated anomalies and uncorrelated anomalies from multiple traces into difference subspaces. After separating correlated and uncorrelated anomalies into different subspace, we can just use our APD algorithm to detect those correlated anomalies. \n",
    "\n",
    "\n",
    "The exact steps for PCA-APD algorithm are as follows:\n",
    "    1. Apply PCA algorithm to multiple traces to capture first K (K=1,2,3) components, and the K >> N (the number of measurements). In our paper, we have shown that the correlated anomalies have a higher probability to be captured in the first K (K=1,2,3) components. And, uncorrelated anomalies may be captured in the rest of the components;\n",
    "    2. Project measurements into first K components;\n",
    "    3. Apply the APD algorithm to projected measurements for detecting correlated anomalies.\n",
    "    \n",
    "(Note: in our orginal paper, we use PCA algoirthm in Matlab for anomaly detection. In this tutorial, we use SVD algorithm, which is simpler and performs similarly as PCA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Prepare the datasets \n",
    "\n",
    "First, we need to prepare our datasets. There are 10 measurements traces used in this tutorial. And, we need to load these 10 traces to construct the a measurement matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traces_path = [ os.getcwd() + '/traffic_dataset/time_series_1.csv',  os.getcwd() + '/traffic_dataset/time_series_2.csv',  os.getcwd() + '/traffic_dataset/time_series_3.csv',\n",
    "                os.getcwd() + '/traffic_dataset/time_series_4.csv',  os.getcwd() + '/traffic_dataset/time_series_5.csv',  os.getcwd() + '/traffic_dataset/time_series_6.csv',\n",
    "                os.getcwd() + '/traffic_dataset/time_series_7.csv',  os.getcwd() + '/traffic_dataset/time_series_8.csv',  os.getcwd() + '/traffic_dataset/time_series_9.csv',\n",
    "                os.getcwd() + '/traffic_dataset/time_series_10.csv']\n",
    "\n",
    "traces_name = ['trace-1', 'trace-2', 'trace-3', 'trace-4', 'trace-5', \n",
    "               'trace-6', 'trace-7', 'trace-8', 'trace-9', 'trace-10']\n",
    "\n",
    "traces_df = list()\n",
    "\n",
    "for i, path in enumerate(traces_path):\n",
    "    # load meassurement into pandas dataframe\n",
    "    df = pd.read_csv(path, header=0, parse_dates=[0])\n",
    "    \n",
    "    # Change the index to timstampe\n",
    "    df = df.set_index(['ts'])\n",
    "    \n",
    "    # remove index name\n",
    "    df.index.name = None\n",
    "    \n",
    "    # append each dataframe to the list\n",
    "    traces_df.append(df)\n",
    "\n",
    "# concatenate all dataframe to a large dataframe, which is 3000 * 10\n",
    "A = pd.concat(traces_df, axis=1)\n",
    "A.columns = traces_name\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the table above, the measurement matrix $A$ is $m \\times n$ matirx, which has 3000 rows and 10 columns; each column denotes a single trace, and each row denotes a certain timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Dimensional reduction through rank-$k$ approximations\n",
    "\n",
    "Lower rank-$k$ approximations is to find lower rank-$k$ ($k < n$) to approximate original high dimensional dataset $A$. Here, we use the Singular Value Decomposition (SVD) to achieve it. (Note, we can use PCA as well).\n",
    "\n",
    "For an $m\\times n$ matrix $A$, the SVD does two things:\n",
    "\n",
    "1. It gives the best rank-$k$ approximation to $A$ for __every__ $k$ up to the rank of $A$.\n",
    "2. It gives the __distance__ of the best approximation $A^{(k)}$ from $A$ for each $k$.\n",
    "\n",
    "\n",
    "The singular value decomposition of a rank-$r$ matrix $A$ has the form:\n",
    "\n",
    "$$A = U\\Sigma V^T$$\n",
    "\n",
    "where,\n",
    "\n",
    "1. Here $r$ is full rank\n",
    "2. $U$ is $m\\times r$\n",
    "3. The columns of $U$ are mutually orthogonal and unit length, ie., $U^TU = I$.\n",
    "4. $V$ is $n\\times r$.\n",
    "5. The columns of $V$ are mutually orthogonal and unit length, ie., $V^TV = I$.\n",
    "6. The matrix $\\Sigma$ is a $r\\times r$ diagonal matrix, whose singular values are $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r > 0$.\n",
    "\n",
    "In terms of the singular value decomposition, \n",
    "\n",
    "1) The distance (in Frobenius norm) of the best rank-$k$ approximation $A^{(k)}$ from $A$ is equal to $\\sqrt{\\sum_{i=k+1}^r\\sigma^2_i}$.\n",
    "\n",
    "2) The best rank-$k$ approximation to $A$ is formed by taking \n",
    "\n",
    "* $U' = $ the $k$ leftmost columns of $U$, \n",
    "* $ \\Sigma' = $ the $k\\times k$ upper left submatrix of $\\Sigma$, and \n",
    "* $V'= $ the $k$ leftmost columns of $V$, and constructing \n",
    "\n",
    "$$ A^{(k)} = U'\\Sigma'(V')^T.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find the best k for approximations\n",
    "\n",
    "We can plot the singular value $\\Sigma$, and each $\\sigma_k$ descirbes the importance of singular vector $u_k$ and $v_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,vt = np.linalg.svd(A)\n",
    "r = len(s)\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1,1+r),s)\n",
    "plt.xlabel(r'$k$',size=20)\n",
    "plt.ylabel(r'$\\sigma_k$',size=20)\n",
    "plt.title(r'Singular Values of $A$',size=20)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform it into ratio\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "Anorm = np.linalg.norm(A)\n",
    "plt.plot(range(1,r + 1),s[0:r]/Anorm)\n",
    "plt.xlim([0,r + 1])\n",
    "plt.xlabel(r'$k$',size=20)\n",
    "plt.ylabel(r'$\\sigma_k$',size=20)\n",
    "print('')\n",
    "s[0:r]/Anorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the singular value $\\sigma_1$ is much more important than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Components and Subspace Analysis\n",
    "\n",
    "In this part, we need detect correlated anomalies. In other words, we need to classify our anomalies into correlated anomalies and uncorrelated anomalies. Lower rank-$k$ approximation (or first K components) can capture common patterns of traffic (or normal traffic). These components form a subspace defined as __normal subspace__.  Anomalies captured in the rest of the components are defined as __anomalous subspace__. In our paper [DRCN2015], we have also demonstrated that the correlated anomalies can be also captured in the __normal subspace__.\n",
    "\n",
    "Let's go through with SVD to understand better of this definition. With SVD, the matrix $A$ can be decomposed into: \n",
    "\n",
    "$$A = U\\Sigma V^T$$\n",
    "\n",
    "And lower rank-$k$ approximation can be written as\n",
    "\n",
    "$$ A^{(k)} = U^{(k)}\\Sigma^{(k)}(V^{(k)})^T.$$\n",
    "\n",
    "where the $k < r$, $r$ is the number of full rank, the $U^{(k)} = \\{u_1, ..., u_k\\}$, $V^{(k)} = \\{v_1, ..., v_k\\}$, and $\\Sigma^{(k)} = \\{\\sigma_1, ..., \\sigma_k\\}$ where $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_k > 0$. Each $u_i$ can be understand as a component, and combinations of $u_i$ construct the subspaces.  \n",
    "\n",
    "We can plot these components to visualize different patterns in different components $u_i$ or different subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,vt = np.linalg.svd(A,full_matrices=False)\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(1,7):\n",
    "    ax = plt.subplot(4,3,i)\n",
    "    plt.plot(u[:,i-1])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(r'$u_%i$' %i, size=20, rotation=0, position=(0, 0.35))\n",
    "plt.subplots_adjust(wspace=0.45,hspace=0.6)\n",
    "plt.suptitle('First Six Columns of $U$',size=20)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows that the traffic captured by the first component $u_1$ is considered a normal pattern. Because it looks more regular than others. However, in $u_i$ we can also find there are __three__ obvious dips captured in $u_1$, which are considered as correlated anomalies. In other components $\\{u_2, ..., $u_6\\}$, we can observe that the pattern is not regular that has random spikes and dips, which are uncorrelated anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "SVD (or PCA) can effectively help us to separate traffic into __normal subspace__ and __anomalous subspace__. \n",
    "\n",
    "- __normal subspace__ : is formed by first $k$ components, correlated anomalies are located at normal subspace\n",
    "- __anomalous subspace__: is formed by rest of components, uncorrelated anomalies are located at anomalous subspace.\n",
    "\n",
    "After separating our traffic matrix $A$ into different subspaces, we can detect relevant anomalies in different subspace using our APD algorithms. Please note that the correlated anomalies can be also captured in anomalous subspace sometimes. But, because we know the which is correlated anomalies in normal subspace, we can easily eliminate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Correlated Anomaly detection in subspace\n",
    "\n",
    "In the last section, we have learned using the low-rank phenomenon to separate anomalous data from the population of normal data. We also learned the concept of subspaces, and how to use SVD to map measurement matrix into __normal subspace__ and __anomalous subspace__. In this section, we will learn how to detect correlated anomalies and uncorrelated anomalies in different subspaces. \n",
    "\n",
    "In practice, here are the basic steps,\n",
    "\n",
    "Given a traffic matrix $A$, we have the following steps:\n",
    "\n",
    "1.  Compute the Singular value decomposition (SVD) of $A$, $$A = U\\Sigma V^T$$ \n",
    "<br /> \n",
    "\n",
    "2.  Choose suitable $k$ for low-rank $k$ approximation to separate traffic into normal subspace and anomalous subspace\n",
    "<br /> \n",
    "\n",
    "\n",
    "3.  Compute normal traffic matrix $N$ can be constructed by using normal subspace $U^{(k)}$, $$N = U^{(k)}\\Sigma^{(k)} (V^{(k)})^T$$ <br /> \n",
    "\n",
    "4.  Compute anomalous traffic matrix $M$:  $$M = A-N$$\n",
    "<br /> \n",
    "\n",
    "5.  Detect correlated anomalies using normal traffic matrix $N$, and detect uncorrelated anomalies using anomalous traffic matrix $M$\n",
    "\n",
    "__Step 1__ and __step 2__ are finised in Section 4.3, and we choose $k=3$ for construct our __normal subspace__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3__: compute our normal traffic matrix $N$, and plot the normal traffic $N$ with its $\\ell_2$ norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unormal = u[:,0:3]            # normal subspace\n",
    "snormal = s[0:3]\n",
    "vnormal = vt[0:3,:]  \n",
    "N = np.dot(np.dot(unormal, np.diag(snormal)), vnormal)\n",
    "Nnorm = np.linalg.norm(N,axis=1)\n",
    "plt.plot(Nnorm)\n",
    "plt.xlabel('Time')\n",
    "plt.title(r'$\\ell_2$ Norm of Normal Traffic')\n",
    "\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, we can observe that there are three correlated anomalies occurred in normal traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4__: compute our anomalous traffic matrix $M$, and plot the anomalous traffic $M$ with its $\\ell_2$ norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = A - N\n",
    "Annorm = np.linalg.norm(M,axis=1)\n",
    "plt.plot(Annorm)\n",
    "plt.xlabel('Time')\n",
    "plt.title(r'$\\ell_2$ Norm of Anomalous Traffic')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure \"$\\ell_2$ Norm of Anomalous Traffic\" shows that there are many uncorrelated anoamlies occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5__: Detect correlated anomalies using normal traffic matrix $N$, and detect uncorrelated anomalies using anomalous traffic matrix $M$.\n",
    "\n",
    "When we get normal traffic matrix $N$ and relevant $\\ell_2$ norm form $Nnorm$, which is one-dimensional dataset, we can just detect correlated anomalies with our APD algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct Nnorm dataframe Nnorm_df\n",
    "Nnorm_df = pd.DataFrame(Nnorm, index=pd.date_range('1/1/2020', freq='5min', periods=3000), columns=['delay'])\n",
    "\n",
    "# TEST FUNCTION: Trigger duration scheme\n",
    "apd = APD(dataset=Nnorm_df)\n",
    "anomalies = apd.detect()\n",
    "\n",
    "# 2. visulize anomlies\n",
    "visualize_anomalies(anomalies, Nnorm_df, 'correlated anomalies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get anomalous traffic matrix $M$ and its $\\ell_2$ norm form $Annorm$, we can just detect uncorrelated anomalies with our APD algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct Nnorm dataframe Nnorm_df\n",
    "Annorm_df = pd.DataFrame(Annorm, index=pd.date_range('1/1/2020', freq='5min', periods=3000), columns=['delay'])\n",
    "\n",
    "# TEST FUNCTION: Trigger duration scheme\n",
    "apd = APD(dataset=Annorm_df)\n",
    "anomalies = apd.detect()\n",
    "\n",
    "# 2. visulize anomlies\n",
    "visualize_anomalies(anomalies, Annorm_df, 'uncorrelated anomalies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5-Conclusion\n",
    "\n",
    "Congrats on finishing this homework about using the idea of dimensional reduction (PCA or SVD) for anomaly detection, understanding the basic theory of low-rank approximation, the concept of subspace, and how to combine SVD and APD to detect correlated and uncorrelated anomalis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "1. Explain the concept of __correlated__ and __uncorrelated__ anomalies in network-wide anomaly detection, and why is the correlated anomalies are more important?\n",
    "2. Explain the concept __normal subspace__ and __anomalous subspace__, explain how to get these two subspaces, and the purpose of the subspace in anomaly detection?\n",
    "3. In our tutorial, we use $k=3$ to separate __normal subspace__ and __anomalous subspace__, please re-produce all the figures in Section 4.3 and section 4.4 using $k=2$. Please submit your executable jupyter notebook file (*.ipynb) with the outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
